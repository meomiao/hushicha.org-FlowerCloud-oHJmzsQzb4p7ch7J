

> 指代表达理解（`REC`）旨在在图像中定位由自由形式自然语言描述指定的目标对象。尽管最先进的方法取得了令人印象深刻的性能，但它们对图像进行了密集感知，包含与语言查询无关的多余视觉区域，导致额外的计算开销。这启发论文探讨一个问题：能否消除与语言无关的多余视觉区域，以提高模型的效率？现有的相关方法主要侧重于基本的视觉任务，在视觉语言领域的探索有限。为了解决这一问题，论文提出了一个称为`ScanFormer`的粗到细的迭代感知框架。该框架逐层利用图像尺度金字塔，从上到下提取与语言相关的视觉图像块。在每次迭代中，通过设计的信息预测方法丢弃不相关的图像块。此外，论文提出了一个用于加速推断的被丢弃图像块的选择策略。在广泛使用的数据集`RefCOCO`、`RefCOCO`\+、`RefCOCOg`和`ReferItGame`上的实验证明了该框架有效性，可以在准确性和效率之间取得平衡。
> 
> 
> 来源：晓飞的算法工程笔记 公众号，转载请注明出处


**论文: ScanFormer: Referring Expression Comprehension by Iteratively Scanning**


![](https://developer.qcloudimg.com/http-save/6496381/4578164bf097ef78cc7fe7c1d9a68784.png)


* **论文地址：[https://arxiv.org/abs/2406\.18048](https://github.com)**


# Introduction




---


   作为视觉语言理解中的基本任务，指代表达理解（`REC`）依赖于自由形式的自然语言描述来识别所指对象。`REC`的发展不仅可以支撑各种视觉语言任务，还有可能有助于实际应用，如人机交互。


![](https://developer.qcloudimg.com/http-save/6496381/25ebfc964330f1cf25e96ac0617838c9.png)


   在指代表达理解（`REC`）中，与高度简洁和信息密集的语言查询形成对比的是，图像通常包含大量冗余信息。例如，如图`1`所示，图像中存在大量与语言查询弱相关甚至不相关的冗余视觉区域，例如目标捕手周围的人物以及大面积的低信息背景区域。然而，最先进的方法采用密集感知形式来获取用于后续跨模态交互的视觉特征。这些方法使用诸如`ResNet`、`DarkNet`、`Swin Transformer`等的视觉编码器，并使用滑动窗口或非重叠图像块遍历图像的所有空间位置以提取特征，如图`1(a)`所示。尽管取得了令人印象深刻的性能，但密集感知的形式带来大量冗余信息，并增加了整个模型的计算开销。特别是在基于`Transformer`的模型中，多头自注意力的计算复杂度是二次的。这引出一个研究问题：是否可能丢弃与语言无关的冗余视觉区域，以提升模型的效率？


   值得注意的是，目前出现了一种探索消除冗余视觉特征的新趋势。典型的自下而上融合方法最初将图像分割成细粒度图像块，并逐渐在后续多个阶段中合并这些图像块以减少视觉标记。然而，初始标记的丰富性不可避免地导致了早期阶段的巨大计算成本，特别是在处理高分辨率图像时。此外，自上而下的粗到细方法从粗粒度分割开始，使用大的图像块尺寸，并逐渐减小图像块尺寸以获取细粒度的视觉标记。例如，`DVT`级联多个`Transformer`，并利用高置信预测来确定是否使用较小的图像块尺寸将整个图像划分为更细粒度的图像块。然而，这种方法通常会带来大量冗余的视觉区域，并增加计算开销。`CF-ViT`引入了一个粗到细的两阶段视觉`Transformer`，该模型在粗阶段识别信息丰富的图像块，并在第二阶段进一步将它们重新分割为更细的图像块。尽管在分类任务中表现出色，但基于类别注意力的启发式信息丰富区域识别限制了其扩展以及在没有\[`CLS`]标记的情况下的建模。此外，由于它是非可学习的，应用正则化来控制标记的稀疏性是具有挑战性的。因此，现有的高效`Transformer`方法仍然存在局限性，并侧重于视觉任务，而忽视了对视觉语言领域的探索。


   为了解决这个问题，论文提出了一种粗到细的迭代感知框架，称为`ScanFormer`，如图`1（b）`所示。具体而言，利用预构建的图像尺度金字塔，模型从金字塔顶部的粗粒度低分辨率图像开始进行视觉感知。通过预测下一次迭代中更细粒度图像块的信息量，模型自适应地消除冗余的视觉区域，最终达到金字塔底部的细粒度高分辨率图像。此外，通过将先前的标记保留在缓存中而不进行进一步的更新（`KVCache`），从而减少计算资源。在每次迭代中提取的新标记，通过自注意力和交叉注意力分别与自身和缓存中包含的先前标记交互。在这个过程中，多尺度图像块分区使模型能够从不同空间位置聚合与尺度相关的信息。此外，论文提出了一种针对被丢弃图像块的图像块选择策略以加速推理。一个可学习的标记参与粗到细的迭代感知过程，并最终用于坐标回归，直接预测目标框。大量实验证明了`ScanFormer`的有效性，在广泛使用的数据集上取得了最先进的方法，即`RefCOCO`，`RefCOCO`\+，`RefCOCOg`和`ReferItGame`。


   主要贡献可以总结如下：


1. 提出了`ScanFormer`，这是一个粗到细的迭代感知框架，每次迭代逐渐丢弃与语言无关的冗余视觉区域，以增强模型的效率。
2. 为了实现图像块选择，提出通过常数标记替换来选择标记，其中未被选中的标记将被常数标记替换，最终合并以真正加速处理。
3. 广泛的实验证明了`ScanFormer`的有效性，与最先进的方法相比，在准确性和效率之间取得了平衡。


# Method




---


![](https://developer.qcloudimg.com/http-save/6496381/58c3249afbf6c87aed72d700b57ec050.png)


## Framework


   `ScanFormer`利用一种统一的类`Transformer`结构，用于语言和视觉模态，如图`2`所示。具体地，该框架由词嵌入、图像块嵌入、位置\-尺度嵌入和编码器组成。词嵌入和图像块嵌入分别从文本和图像中提取特征。位置\-尺度嵌入用于编码每个图像图像块的空间位置和尺度大小。编码器由`N`个层组成，每个层包括一个多头注意力（`MHA`）层和一个前馈网络（`FFN`）。


![](https://developer.qcloudimg.com/http-save/6496381/bd0242098380dad2aaef76dfe0c99a7f.png)


   此外，每个编码器层都配备有一个缓存来存储输出特征。`MHA`的查询来自输入特征，而键和值由输入特征和先前缓存特征组成，如图`3`所示。尺度上的因果性不仅减少了计算量，还利用了先前的语言和多尺度视觉信息来更新特征。


   语言模态的输入首先由该框架进行编码，并提取的语言特征被存储在缓存中。随后，对于视觉模态，基于输入图像 \\(I\\) 构建了一个包含 \\(S\\) 个尺度的图像尺度金字塔。自顶向下，对于每次迭代，选择的图像块被提取并通过框架进行处理，中间特征用于生成下一个金字塔层中子图像块的选择。此外，每个编码器层的缓存存储每次迭代后获得的视觉特征。每次迭代中与\[`REG`]标记对应的特征用于预测对应尺度上指代对象的坐标。对于金字塔顶部的图像，选择所有图像块以确保模型捕获全局信息。随着尺度的增加，`ScanFormer`融合了更精细的特征以实现准确的预测，同时丢弃不相关的图像块以节省大量计算资源。


   具体而言，对于语言模态，指代文本 \\(t\\in\\mathbb{R}^{L\\times\|V\|}\\) 被嵌入到词嵌入矩阵 \\(T\\in\\mathbb{R}^{\|V\|\\times d}\\) 中，前置\[`CLS`]嵌入 \\(T^{cls}\\in\\mathbb{R}^d\\) ，然后与文本位置嵌入矩阵 \\(T^{pos}\\in\\mathbb{R}^{(L\+1\)\\times d}\\) 和类型嵌入 \\(T^{type}\\in\\mathbb{R}^d\\) 相加。嵌入的语言特征首先被送入框架中，更新后的语言特征将存储在编码器每层的缓存中。对于视觉模态，在图像尺度金字塔的自顶向下，以第 \\(i\\) 层为例，首先从具有 \\((P, P)\\) 分辨率和 \\(C\\) 通道的 \\(N\_i\\) 个选择的图像块被平铺为 \\(v\\in\\mathbb{R}^{N\_i\\times (P^2\\cdot C)}\\) ，然后通过线性投影层投影到 \\(E\\in\\mathbb{R}^{N\_i \\times d}\\) 。之后，图像块特征与空间嵌入 \\(E^{spatial}\\in\\mathbb{R}^{N\_i\\times d}\\) 和类型嵌入 \\(E^{type}\\in\\mathbb{R}^d\\) 相加。 \\(E^{spatial}\\) 由位置\-尺度嵌入 \\(PSE: \[0,1]^3 \\rightarrow \\mathbb{R}^d\\) 生成，使用规范化的图像块坐标和尺度 \\(\[cx, cy, s]\\) 作为输入。之后，附加\[`REG`]标记的嵌入 \\(E^{reg}\\in\\mathbb{R}^d\\) 用于回归第 \\(i\\) 层对象的边界框 \\(\[cx\_i, cy\_i, w\_i, h\_i]\\) 。


## Patch Selection by Constant Replacement


   为了通过反向传播学习选择信息图像块，为第 \\(i\\) 个图像块生成了选择因子 \\(s\_i\\) 。关于如何使用 \\(s\_i\\) 有两个选择：（`1`）将 \\(s\_i\\) 应用于每个`Transformer`层上`MHA`的每个头，通过对键和值进行加权实现。逐渐将 \\(s\_i\\) 衰减为 \\(0\.0\\) ，以最小化其对其余标记的影响。然而，对于具有 \\(N\\) 层和 \\(H\\) 头的`Transformer`，获得清晰的梯度信号以优化 \\(s\_i\\) 是具有挑战性的，使得难以实现理想的学习选择。（`2`）将 \\(s\_i\\) 直接加权地应用于`Transformer`的输入，即图像块嵌入。由于 \\(s\_i\\) 仅在此位置使用，因此更容易训练。因此，本文采用了第二种选择。


   此外，值得注意的是，即使将输入图像块嵌入设置为零，由于`FFN`和`MHA`的偏置项以及点积注意力，它在后续层中仍会变为非零。幸运的是，当标记序列包含许多相同的标记时，`MHA`的计算可以被简化，从而实现实际推理加速。为了提高模型的适应性，论文建议将图像块嵌入替换为可学习的常数标记，而不是直接将其设置为零。因此，图像块选择问题被转化为图像块替换问题。


* ### Constant Token Replacement


   为了实现标记替换，引入了一个常数标记 \\(E^{const}\\in\\mathbb{R}^d\\) ，并从`Transformer`中产生第 \\(i\\) 个图像块的选择值 \\(r\_i\\in \\mathbb{R}\\) 。遵循改进的语义哈希方法，通过反向传播学习 \\(r\_i\\) 。为了鼓励探索，将噪声添加到 \\(r\_i\\) 中，即 \\(r\_i^n\=r\_i\+n\\) 。在训练过程中， \\(n\\sim \\mathcal{N}(0,1\)\\) ，而在评估和推断时 \\(n\=0\\) 。然后，计算两个变量 \\(v\_1\=\\sigma{'}(r\_i^n)\\) 和 \\(v\_2\=\\mathbb{I}(r\_i^n\\geq 0\)\\) 。


\\\[\\begin{equation}
\\sigma{'}(x) \= clamp(1\.2\\sigma(x)\-0\.1, 0, 1\),
\\end{equation}
\\]   其中， \\(\\mathbb{I}(\\cdot)\\) 和 \\(\\sigma(\\cdot)\\) 分别为指示函数和 \\(sigmoid\\) 函数。在训练过程中，在前向传播中，均匀抽样 \\(v\_1\\) 和 \\(v\_2\\) 作为选择因子 \\(s\_i\\) 。


\\\[\\begin{equation}
\\label{equ:select}
s\_i \= \\mathbb{I}(n\_s\\geq0\.5\)\\cdot v\_1 \+\\mathbb{I}(n\_s \< 0\.5\)\\cdot v\_2,
\\end{equation}
\\]   这里， \\(n\_s\\sim Uniform\[0, 1]\\) 表示随机采样权重。在反向传播中，梯度始终流向 \\(v\_1\\) ，即使在前向计算中使用了 \\(v\_2\\) 。加权图像块嵌入 \\(\\overline{E}\_i\\) 计算如下：


\\\[\\begin{equation}
\\overline{E}\_i \= s\_i\\cdot E\_i \+ (1\-s\_i)\\cdot E^{const}.
\\end{equation}
\\]   在训练过程中， \\(s\_i\\) 被规范化为 \\(0\\) ，即第 \\(i\\) 个标记被常量标记 \\(E^{const}\\) 替换。


* ### Merging Constant Tokens


![](https://developer.qcloudimg.com/http-save/6496381/2368b5f088df2159c40f01d174856b01.png)


   尽管冗余标记被常量标记替换后仍然包含在编码器的前向计算中，但它们不能直接被丢弃而不产生任何影响。然而，这些常量标记可以合并以有效减少计算量。对于一个包含 \\(N\\) 个标记和 \\(N\_c\\) 个常量标记的键和值序列：


\\\[\\begin{equation}
\\label{equ:orig\_kv}
\\begin{split}
\&K\=\[\\underbrace{k\_1, k\_2, \\cdots, k\_i}\_{N\-N\_c}, \\underbrace{k^c,\\cdots, k^c}\_{N\_c}] \\\\
\&V\=\[\\underbrace{v\_1, v\_2, \\cdots, v\_i}\_{N\-N\_c}, \\underbrace{v^c,\\cdots, v^c}\_{N\_c}]
\\end{split}
\\end{equation}
\\]   通过将一个常量向量连接到键上， \\(N\_c\\) 个标记的键和值可以减少到仅一个键和一个值，这可以通过以下方式来说明：


\\\[\\begin{equation}
\\label{equ:merge\_kv}
\\begin{split}
\&K^{'}\=concat(\[\\underbrace{k\_1, k\_2, \\cdots, k\_i}\_{N\-N\_c}, k^c], \[\\underbrace{0, 0, \\cdots, 0}\_{N\-N\_c}, log(N\_c)]) \\\\
\&V^{'}\=\[\\underbrace{v\_1, v\_2, \\cdots, v\_i}\_{N\-N\_c}, v^c]
\\end{split}
\\end{equation}
\\]   根据缩放点积注意力机制，相对于 \\(K\\) 的一个查询 \\(q\\in\\mathbb{R}^{d}\\) 的注意力值 \\(A\\in\\mathbb{R}^{N}\\) 可以计算为：


\\\[\\begin{equation}
\\label{equ:attn}
A \= softmax(\\frac{qK^T}{\\sqrt{d}}).
\\end{equation}
\\]   根据公式`4`和公式`5`，可以得出同样的注意力加权值，如公式`6`所示。因此， \\(N\_c\-1\\) 个标记最终被丢弃，由它们带来的计算可以节省。


## Prediction Head


   被指代的对象可能存在于各种尺度上。类似于目标检测方法，其中在不同特征级别进行多尺度预测。对于`ScanFormer`中的每个尺度级别，应用直接坐标回归来预测被指代对象的边界框，通过回归标记\[`REG`]来搜集`Transformer`中图像图像块的特征。与\[`REG`]标记对应的输出特征被馈送到共享的多层感知器(`MLP`)，随后使用`Sigmoid`函数来预测被引用对象的归一化边界框 \\(\\hat{b}\=(\\hat{x},\\hat{y},\\hat{w},\\hat{h})\\) 。


## Training Objectives


   通过端到端的方式优化所提出的粗到细的迭代感知框架。对于第 \\(l\\) 个图像尺度，可以获得预测的边界框 \\(\\hat{b}\_l\=(\\hat{x}\_l,\\hat{y}\_l,\\hat{w}\_l,\\hat{h}\_l)\\) 。给定真实边界框 \\(b\=(x,y,w,h)\\) ，检测损失函数定义如下：


\\\[\\begin{equation}
\\label{equ:loss\_bbox}
\\mathcal{L}\_{bbox} \= \\sum\_{l\=0}^{2} \\lambda\_{L1}^l\\mathcal{L}\_{L1}(b,\\hat{b}\_l) \+ \\sum\_{l\=0}^{2} \\lambda\_{giou}^l\\mathcal{L}\_{giou}^l(b,\\hat{b}\_l),
\\end{equation}
\\]   其中， \\(\\mathcal{L}\_{L1}(\\cdot,\\cdot)\\) 和 \\(\\mathcal{L}\_{giou}(\\cdot,\\cdot)\\) 分别代表`L1`损失和广义`IoU`损失，而 \\(\\lambda\_{L1}^l\\) 和 \\(\\lambda\_{giou}^l\\) 分别是相对权重，用于控制第 \\(l\\) 个图像尺度上的检测损失惩罚。


   此外，为了控制所选图像块的稀疏性，添加正则化损失函数如下：


\\\[\\begin{equation}
\\label{equ:loss\_sparse}
\\mathcal{L}\_{sparse} \= \\lambda\_{sparse}\\sum\_{l\=1}^{2}(\\frac{1}{N\_l}\\sum\_{i\=1}^{N\_l}s\_i^l\-\\beta^l)^2,
\\end{equation}
\\]   其中， \\(\\lambda\_{sparse}\\) 代表相对权重，用于控制稀疏性惩罚， \\(s\_i^l\\) 代表第 \\(l\\) 个图像尺度中公式`2`中的第 \\(i\\) 个图像块的选择因子。 \\(\\beta^l\\) 是用于控制从第 \\(l\\) 个图像尺度选择的标记比例的超参数。


   `ScanFormer`的总损失函数定义如下：


\\\[\\begin{equation}
\\mathcal{L}\_{total} \= \\mathcal{L}\_{bbox} \+ \\mathcal{L}\_{sparse},
\\end{equation}
\\]   经过训练的`ScanFormer`可以在准确性和效率之间取得平衡。


# Experiment




---


![](https://developer.qcloudimg.com/http-save/6496381/ed932bd6cb5dbb15042350f1b0c1a3cd.png)


![](https://developer.qcloudimg.com/http-save/6496381/3e7bcbb65c7752bff5e00e9c9339352e.png)


![](https://developer.qcloudimg.com/http-save/6496381/ee689b463f8c474d083d0963061c454d.png)


![](https://developer.qcloudimg.com/http-save/6496381/d5f5cd85455c72938522ee74365fff31.png)


![](https://developer.qcloudimg.com/http-save/6496381/3c209cfafa837f6b54ae048d09941c14.png)


![](https://developer.qcloudimg.com/http-save/6496381/04fed533f8a124039be6f20620c769cf.png)


 
 
 



> 如果本文对你有帮助，麻烦点个赞或在看呗～
> 更多内容请关注 微信公众号【晓飞的算法工程笔记】


![work-life balance.](https://upload-images.jianshu.io/upload_images/20428708-7156c0e4a2f49bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
 本博客参考[飞数机场](https://ze16.com)。转载请注明出处！
